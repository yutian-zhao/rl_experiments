{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from ppo import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoyutian/Documents/nlp/lib/python3.10/site-packages/gymnasium/vector/__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/zhaoyutian/Documents/nlp/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# environment hyperparams\n",
    "n_envs = 10\n",
    "n_updates = 1000\n",
    "n_steps_per_update = 128\n",
    "nbatch = n_envs * n_steps_per_update\n",
    "batch_size = 32\n",
    "nbatch_train = nbatch // batch_size\n",
    "n_epochs = 4\n",
    "\n",
    "# agent hyperparams\n",
    "epsilon = 0.2\n",
    "gamma = 0.999\n",
    "lam = 0.95  # hyperparameter for GAE\n",
    "ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "envs = gym.vector.make(\"LunarLander-v2\", num_envs=n_envs, max_episode_steps=600)\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [40] cannot be broadcast to indexing result of shape [0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m nbatch_train\n\u001b[1;32m     55\u001b[0m     mbinds \u001b[38;5;241m=\u001b[39m inds[start:end]\n\u001b[0;32m---> 57\u001b[0m critic_loss, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_rewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_action_log_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_value_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mep_entropies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmbinds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# update the actor and critic networks\u001b[39;00m\n\u001b[1;32m     74\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate_parameters(critic_loss, actor_loss)\n",
      "File \u001b[0;32m~/Documents/rl_experiments/ppo.py:194\u001b[0m, in \u001b[0;36mA2C.get_losses\u001b[0;34m(self, states, actions, rewards, action_log_probs_k, value_preds, entropy, masks, gamma, lam, epsilon, ent_coef, device, n_envs)\u001b[0m\n\u001b[1;32m    192\u001b[0m clip_ratios[advantages \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mepsilon)\n\u001b[1;32m    193\u001b[0m clipped_pg_loss \u001b[38;5;241m=\u001b[39m clip_ratios \u001b[38;5;241m*\u001b[39m advantages\u001b[38;5;241m.\u001b[39mdetach() \n\u001b[0;32m--> 194\u001b[0m \u001b[43mpg_loss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclipped_pg_loss\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43mpg_loss\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m clipped_pg_loss\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, pg_loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mshape, entropy\u001b[38;5;241m.\u001b[39mshape, entropy\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mshape())\n\u001b[1;32m    198\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;241m-\u001b[39mpg_loss\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m ent_coef \u001b[38;5;241m*\u001b[39m entropy\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    200\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [40] cannot be broadcast to indexing result of shape [0]"
     ]
    }
   ],
   "source": [
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    # reset lists that collect experiences of an episode (sample phase)\n",
    "    ep_states = torch.zeros(n_steps_per_update*n_envs, obs_shape, device=device)\n",
    "    ep_actions = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "    ep_entropies = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update*n_envs, device=device)\n",
    "\n",
    "    # at the start of training reset all envs to get an initial state\n",
    "    if sample_phase == 0:\n",
    "        states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "    # play n steps in our parallel environments to collect data\n",
    "    for step in range(n_steps_per_update):\n",
    "        ep_states[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.tensor(states, device=device)\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(\n",
    "            states\n",
    "        )\n",
    "        \n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(\n",
    "            actions.cpu().numpy()\n",
    "        )\n",
    "\n",
    "        ep_value_preds[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = action_log_probs\n",
    "        ep_actions[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.squeeze(actions)\n",
    "        ep_entropies[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.squeeze(entropy)\n",
    "\n",
    "        # add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        masks[sample_phase*n_steps_per_update+step:sample_phase*n_steps_per_update+step+n_envs] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "    # calculate the losses for actor and critic\n",
    "    inds = np.arange(nbatch)\n",
    "    for _ in range(n_epochs):\n",
    "        # Randomize the indexes\n",
    "        np.random.shuffle(inds)\n",
    "        # 0 to batch_size with batch_train_size step\n",
    "        for start in range(0, nbatch, nbatch_train):\n",
    "            end = start + nbatch_train\n",
    "            mbinds = inds[start:end]\n",
    "\n",
    "        critic_loss, actor_loss = agent.get_losses(\n",
    "            ep_states[mbinds],\n",
    "            ep_actions[mbinds],\n",
    "            ep_rewards[mbinds],\n",
    "            ep_action_log_probs[mbinds],\n",
    "            ep_value_preds[mbinds],\n",
    "            ep_entropies[mbinds],\n",
    "            masks[mbinds],\n",
    "            gamma,\n",
    "            lam,\n",
    "            epsilon,\n",
    "            ent_coef,\n",
    "            device,\n",
    "            n_envs,\n",
    "        )\n",
    "\n",
    "        # update the actor and critic networks\n",
    "        agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    agent.sync_actor()\n",
    "\n",
    "    # log the losses and entropy\n",
    "    # TODO: change to multi-batch \n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().mean().cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Plotting\n",
    "# --------\n",
    "#\n",
    "\n",
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the LunarLander-v2 environment \\n \\\n",
    "             (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
