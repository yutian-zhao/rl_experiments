{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from array2gif import write_gif\n",
    "from minigrid.wrappers import ImgObsWrapper, FullyObsWrapper\n",
    "\n",
    "\n",
    "import gym_examples\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_skills = 4\n",
    "skill_eps_len = 10  # 90 10\n",
    "skill_eps_count = 10\n",
    "num_episodes = 1000  # 600\n",
    "discount = 0.9\n",
    "final_step_discount = 0.99\n",
    "lr_pred = 1e-4\n",
    "lr_reg = 1e-4\n",
    "lr_policy = 1e-4\n",
    "lr_extractor = 1e-4\n",
    "BATCH_SIZE = 64  # 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 50000\n",
    "TAU = 0.005\n",
    "features_dim = 64  # 128\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoyutian/Documents/nlp/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.width to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.width` for environment variables or `env.get_wrapper_attr('width')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/zhaoyutian/Documents/nlp/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.height to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.height` for environment variables or `env.get_wrapper_attr('height')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# seaquest gpu out of memory\n",
    "# env = gym.make('gym_examples/GridWorld-v0', size=10, render_mode=\"rgb_array\")\n",
    "# env = gym.make('gym_examples:gym_examples/GridWorld-v0', size=10, render_mode=\"rgb_array\")\n",
    "# env = gym_examples.AgentLocation(env)\n",
    "# env = gym_examples.FullFrame(env)\n",
    "\n",
    "env = gym.make(\"MiniGrid-FourRooms-v0\", render_mode=\"rgb_array\")\n",
    "env = FullyObsWrapper(env)\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "# obs, info = env.reset()\n",
    "# print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeaturesExtractor(\n",
    "    env.observation_space, features_dim=features_dim, concat=1).to(device)\n",
    "\n",
    "predictor_net = Head(features_dim*2, num_skills, if_prob=True).to(device)\n",
    "regularizer_net = Head(features_dim, num_skills, if_prob=True).to(device)\n",
    "prev_predictor_net = Head(features_dim*2, num_skills, if_prob=True).to(device)\n",
    "prev_regularizer_net = Head(features_dim, num_skills, if_prob=True).to(device)\n",
    "\n",
    "policy_net = Head(features_dim+1, n_actions).to(device)\n",
    "target_net = Head(features_dim+1, n_actions).to(device)\n",
    "\n",
    "prev_predictor_net.load_state_dict(predictor_net.state_dict())\n",
    "prev_regularizer_net.load_state_dict(regularizer_net.state_dict())\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [{'params': policy_net.parameters(), 'lr': lr_policy},\n",
    "     {'params': predictor_net.parameters(), 'lr': lr_pred},\n",
    "     {'params': regularizer_net.parameters(), 'lr': lr_reg},\n",
    "     {'params': feature_extractor.parameters(), 'lr': lr_extractor}], amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, skill, steps_done):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state, skill).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_error():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([\n",
    "        feature_extractor(torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0).permute((0, 3, 1, 2))) \n",
    "        for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat([\n",
    "        feature_extractor(torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0).permute((0, 3, 1, 2))) \n",
    "        for s in batch.state])\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    # print(batch.reward)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    skill_batch = torch.cat(batch.skill)\n",
    "    state_action_values = policy_net(\n",
    "        state_batch, skill_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(\n",
    "            non_final_next_states, skill_batch[non_final_mask]).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    td_error = criterion(state_action_values,\n",
    "                         expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # # Optimize the model\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # # In-place gradient clipping\n",
    "    # torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    # optimizer.step()\n",
    "\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "steps_done = 0\n",
    "predictor_update_preiod = 10\n",
    "td_error_list = []\n",
    "reward_list = []\n",
    "pred_loss_list = []\n",
    "reg_loss_list = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state_feature = feature_extractor(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(\n",
    "        0).permute((0, 3, 1, 2)))  # (N, W, H, C) -> (N, D)\n",
    "    for i_skil_eps in range(skill_eps_count):\n",
    "        skill = torch.tensor([random.choice([*range(num_skills)])], device=device).unsqueeze(\n",
    "            0).repeat(((state_feature.size()[0]), 1))  # [1, 1]\n",
    "        # skill_state_feature = torch.cat((skill, state_feature), dim=1) # (N, D+1)\n",
    "        terminal_state = None\n",
    "        secondary_memory = []\n",
    "        for t in range(skill_eps_len):\n",
    "            action = select_action(state_feature, skill, steps_done)\n",
    "            steps_done += 1\n",
    "            observation, _, terminated, truncated, _ = env.step(\n",
    "                action.item())  # zero discount? ignore reward\n",
    "            # reward = nn.functional.softmax(predictor_net()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            next_state = observation\n",
    "            next_state_feature = feature_extractor(torch.tensor(\n",
    "                observation, dtype=torch.float32, device=device).unsqueeze(0).permute((0, 3, 1, 2)))\n",
    "\n",
    "            if terminated:\n",
    "                secondary_memory.append((state, action, None, skill))\n",
    "                state = next_state\n",
    "                state_feature = next_state_feature\n",
    "                break\n",
    "            elif truncated:\n",
    "                secondary_memory.append((state, action, next_state, skill))\n",
    "                state = next_state\n",
    "                state_feature = next_state_feature\n",
    "                break\n",
    "            else:\n",
    "                secondary_memory.append((state, action, next_state, skill))\n",
    "                state = next_state\n",
    "                state_feature = next_state_feature\n",
    "\n",
    "        initial_state = secondary_memory[0][0]\n",
    "        initial_state_feature = feature_extractor(torch.tensor(\n",
    "            state, dtype=torch.float32, device=device).unsqueeze(0).permute((0, 3, 1, 2)))\n",
    "        init_terminal_feature = torch.cat(\n",
    "            (initial_state_feature, state_feature), dim=1)\n",
    "        with torch.no_grad():\n",
    "            reward = prev_predictor_net(init_terminal_feature)[\n",
    "                :, skill.item()] - prev_regularizer_net(state_feature)[:, skill.item()]\n",
    "            # print(reward)\n",
    "\n",
    "        for s, a, ns, sk in secondary_memory:\n",
    "            memory.push(s, a, ns, sk, reward)\n",
    "\n",
    "        td_error = compute_td_error()\n",
    "\n",
    "        pred_loss = - \\\n",
    "            torch.log(predictor_net(init_terminal_feature))[:, skill.item()]\n",
    "        # predictor_optimizer.zero_grad()\n",
    "        # pred_loss.backward()\n",
    "        # predictor_optimizer.step()\n",
    "        # Following paper, cross entropy is not used\n",
    "        reg_loss = - torch.log(regularizer_net(state_feature))[:, skill.item()]\n",
    "        # regularizer_optimizer.zero_grad()\n",
    "        # reg_loss.backward(retain_graph=True)\n",
    "        # regularizer_optimizer.step()\n",
    "\n",
    "        if td_error:\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * \\\n",
    "                    TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "            # TODO: parallelizable?\n",
    "            loss = td_error + pred_loss + reg_loss\n",
    "            td_error_list.append(td_error)\n",
    "        else:\n",
    "            loss = pred_loss + reg_loss\n",
    "            td_error_list.append(torch.tensor(0))\n",
    "\n",
    "        reward_list.append(reward)\n",
    "        pred_loss_list.append(pred_loss)\n",
    "        reg_loss_list.append(reg_loss)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        if td_error:\n",
    "            torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "        predictor_update_preiod -= 1\n",
    "        if not predictor_update_preiod:\n",
    "            prev_predictor_net.load_state_dict(predictor_net.state_dict())\n",
    "            prev_regularizer_net.load_state_dict(regularizer_net.state_dict())\n",
    "            predictor_update_preiod = 10\n",
    "\n",
    "        if done:\n",
    "            # TODO: caution of breaking nested loop\n",
    "            break\n",
    "\n",
    "    if i_episode > 0 and not i_episode % 100:\n",
    "        torch.save(target_net.state_dict(), 'target_net_{}.dict'.format(i_episode))\n",
    "        torch.save(feature_extractor.state_dict(), 'feature_extractor_{}.dict'.format(i_episode))\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('gym_examples/GridWorld-v0', size=10, render_mode=\"rgb_array\")\n",
    "# # env = gym.make('gym_examples:gym_examples/GridWorld-v0', size=10, render_mode=\"rgb_array\")\n",
    "# env = gym_examples.FullFrame(env)\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"eval\", name_prefix=\"eval\",\n",
    "#                                episode_trigger=lambda x: True)\n",
    "# num_eval_episodes = 1\n",
    "\n",
    "# for episode_num in range(num_eval_episodes):\n",
    "#     obs, _ = env.reset()\n",
    "\n",
    "#     for t in range(100):\n",
    "\n",
    "#         obs = feature_extractor(torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(\n",
    "#             0).permute((0, 3, 1, 2)))\n",
    "#         action = target_net(obs, torch.tensor(\n",
    "#             [[3]], device=device)).max(1).indices.view(1, 1)\n",
    "#         print(action)\n",
    "#         obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "#         done = terminated | truncated\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "# env.render()\n",
    "\n",
    "for episode in range(1):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    for t in range(40):\n",
    "        # env.render()\n",
    "\n",
    "        frames.append(np.moveaxis(env.get_frame(), 2, 0))\n",
    "        obs = feature_extractor(torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).permute((0, 3, 1, 2)))\n",
    "        action = target_net(obs, torch.tensor(\n",
    "            [[3]], device=device)).max(1).indices.view(1, 1)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated | truncated\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "write_gif(np.array(frames), \"test0.gif\", fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
